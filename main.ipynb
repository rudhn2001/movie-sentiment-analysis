{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re # for regex\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./IMDB-Dataset.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "5  Probably my all-time favorite movie, a story o...          1\n",
       "6  I sure would like to see a resurrection of a u...          1\n",
       "7  This show was an amazing, fresh & innovative i...          0\n",
       "8  Encouraged by the positive comments about this...          0\n",
       "9  If you like original gut wrenching laughter yo...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.replace('positive',1,inplace=True)\n",
    "data.sentiment.replace('negative',0,inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    cleaned = re.compile(r'<.*?>')\n",
    "    return re.sub(cleaned,'',text)\n",
    "\n",
    "data.review = data.review.apply(clean)\n",
    "data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked  They are right  as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many  Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more    so scuffles  death stares  dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance   OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing    thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_special(text):\n",
    "    rem = ''\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            rem = rem + i\n",
    "        else:\n",
    "            rem = rem + ' '\n",
    "    return rem\n",
    "\n",
    "data.review = data.review.apply(is_special)\n",
    "data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the other reviewers has mentioned that after watching just 1 oz episode you ll be hooked  they are right  as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence  which set in right from the word go  trust me  this is not a show for the faint hearted or timid  this show pulls no punches with regards to drugs  sex or violence  its is hardcore  in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary  it focuses mainly on emerald city  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  em city is home to many  aryans  muslims  gangstas  latinos  christians  italians  irish and more    so scuffles  death stares  dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  forget pretty pictures painted for mainstream audiences  forget charm  forget romance   oz doesn t mess around  the first episode i ever saw struck me as so nasty it was surreal  i couldn t say i was ready for it  but as i watched more  i developed a taste for oz  and got accustomed to the high levels of graphic violence  not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  watching oz  you may become comfortable with what is uncomfortable viewing    thats if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "data.review = data.review.apply(to_lower)\n",
    "data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[1;32m----> 6\u001b[0m data\u001b[39m.\u001b[39mreview \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mreview\u001b[39m.\u001b[39;49mapply(rem_stopwords)\n\u001b[0;32m      7\u001b[0m data\u001b[39m.\u001b[39mreview[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn [15], line 2\u001b[0m, in \u001b[0;36mrem_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrem_stopwords\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "data.review = data.review.apply(rem_stopwords)\n",
    "data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o n e   o f   t h e   o t h e r   r e v i e w e r s   h a s   m e n t i o n e d   t h a t   a f t e r   w a t c h i n g   j u s t   1   o z   e p i s o d e   y o u   l l   b e   h o o k e d     t h e y   a r e   r i g h t     a s   t h i s   i s   e x a c t l y   w h a t   h a p p e n e d   w i t h   m e   t h e   f i r s t   t h i n g   t h a t   s t r u c k   m e   a b o u t   o z   w a s   i t s   b r u t a l i t y   a n d   u n f l i n c h i n g   s c e n e s   o f   v i o l e n c e     w h i c h   s e t   i n   r i g h t   f r o m   t h e   w o r d   g o     t r u s t   m e     t h i s   i s   n o t   a   s h o w   f o r   t h e   f a i n t   h e a r t e d   o r   t i m i d     t h i s   s h o w   p u l l s   n o   p u n c h e s   w i t h   r e g a r d s   t o   d r u g s     s e x   o r   v i o l e n c e     i t s   i s   h a r d c o r e     i n   t h e   c l a s s i c   u s e   o f   t h e   w o r d   i t   i s   c a l l e d   o z   a s   t h a t   i s   t h e   n i c k n a m e   g i v e n   t o   t h e   o s w a l d   m a x i m u m   s e c u r i t y   s t a t e   p e n i t e n t a r y     i t   f o c u s e s   m a i n l y   o n   e m e r a l d   c i t y     a n   e x p e r i m e n t a l   s e c t i o n   o f   t h e   p r i s o n   w h e r e   a l l   t h e   c e l l s   h a v e   g l a s s   f r o n t s   a n d   f a c e   i n w a r d s     s o   p r i v a c y   i s   n o t   h i g h   o n   t h e   a g e n d a     e m   c i t y   i s   h o m e   t o   m a n y     a r y a n s     m u s l i m s     g a n g s t a s     l a t i n o s     c h r i s t i a n s     i t a l i a n s     i r i s h   a n d   m o r e         s o   s c u f f l e s     d e a t h   s t a r e s     d o d g y   d e a l i n g s   a n d   s h a d y   a g r e e m e n t s   a r e   n e v e r   f a r   a w a y   i   w o u l d   s a y   t h e   m a i n   a p p e a l   o f   t h e   s h o w   i s   d u e   t o   t h e   f a c t   t h a t   i t   g o e s   w h e r e   o t h e r   s h o w s   w o u l d n   t   d a r e     f o r g e t   p r e t t y   p i c t u r e s   p a i n t e d   f o r   m a i n s t r e a m   a u d i e n c e s     f o r g e t   c h a r m     f o r g e t   r o m a n c e       o z   d o e s n   t   m e s s   a r o u n d     t h e   f i r s t   e p i s o d e   i   e v e r   s a w   s t r u c k   m e   a s   s o   n a s t y   i t   w a s   s u r r e a l     i   c o u l d n   t   s a y   i   w a s   r e a d y   f o r   i t     b u t   a s   i   w a t c h e d   m o r e     i   d e v e l o p e d   a   t a s t e   f o r   o z     a n d   g o t   a c c u s t o m e d   t o   t h e   h i g h   l e v e l s   o f   g r a p h i c   v i o l e n c e     n o t   j u s t   v i o l e n c e     b u t   i n j u s t i c e     c r o o k e d   g u a r d s   w h o   l l   b e   s o l d   o u t   f o r   a   n i c k e l     i n m a t e s   w h o   l l   k i l l   o n   o r d e r   a n d   g e t   a w a y   w i t h   i t     w e l l   m a n n e r e d     m i d d l e   c l a s s   i n m a t e s   b e i n g   t u r n e d   i n t o   p r i s o n   b i t c h e s   d u e   t o   t h e i r   l a c k   o f   s t r e e t   s k i l l s   o r   p r i s o n   e x p e r i e n c e     w a t c h i n g   o z     y o u   m a y   b e c o m e   c o m f o r t a b l e   w i t h   w h a t   i s   u n c o m f o r t a b l e   v i e w i n g         t h a t s   i f   y o u   c a n   g e t   i n   t o u c h   w i t h   y o u r   d a r k e r   s i d e  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_txt(text):\n",
    "    ss = SnowballStemmer('english')\n",
    "    return \" \".join([ss.stem(w) for w in text])\n",
    "\n",
    "data.review = data.review.apply(stem_txt)\n",
    "data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m      3\u001b[0m cv \u001b[39m=\u001b[39m CountVectorizer(max_features \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m X \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mfit_transform(data\u001b[39m.\u001b[39;49mreview)\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mX.shape = \u001b[39m\u001b[39m\"\u001b[39m,X\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39my.shape = \u001b[39m\u001b[39m\"\u001b[39m,y\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1283\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1284\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m         )\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "X = np.array(data.iloc[:,0].values)\n",
    "y = np.array(data.sentiment.values)\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "X = cv.fit_transform(data.review).toarray()\n",
    "print(\"X.shape = \",X.shape)\n",
    "print(\"y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o n e   o f   t h e   o t h e r   r e v i e w e r s   h a s   m e n t i o n e d   t h a t   a f t e r   w a t c h i n g   j u s t   1   o z   e p i s o d e   y o u   l l   b e   h o o k e d     t h e y   a r e   r i g h t     a s   t h i s   i s   e x a c t l y   w h a t   h a p p e n e d   w i t h   m e   t h e   f i r s t   t h i n g   t h a t   s t r u c k   m e   a b o u t   o z   w a s   i t s   b r u t a l i t y   a n d   u n f l i n c h i n g   s c e n e s   o f   v i o l e n c e     w h i c h   s e t   i n   r i g h t   f r o m   t h e   w o r d   g o     t r u s t   m e     t h i s   i s   n o t   a   s h o w   f o r   t h e   f a i n t   h e a r t e d   o r   t i m i d     t h i s   s h o w   p u l l s   n o   p u n c h e s   w i t h   r e g a r d s   t o   d r u g s     s e x   o r   v i o l e n c e     i t s   i s   h a r d c o r e     i n   t h e   c l a s s i c   u s e   o f   t h e   w o r d   i t   i s   c a l l e d   o z   a s   t h a t   i s   t h e   n i c k n a m e   g i v e n   t o   t h e   o s w a l d   m a x i m u m   s e c u r i t y   s t a t e   p e n i t e n t a r y     i t   f o c u s e s   m a i n l y   o n   e m e r a l d   c i t y     a n   e x p e r i m e n t a l   s e c t i o n   o f   t h e   p r i s o n   w h e r e   a l l   t h e   c e l l s   h a v e   g l a s s   f r o n t s   a n d   f a c e   i n w a r d s     s o   p r i v a c y   i s   n o t   h i g h   o n   t h e   a g e n d a     e m   c i t y   i s   h o m e   t o   m a n y     a r y a n s     m u s l i m s     g a n g s t a s     l a t i n o s     c h r i s t i a n s     i t a l i a n s     i r i s h   a n d   m o r e         s o   s c u f f l e s     d e a t h   s t a r e s     d o d g y   d e a l i n g s   a n d   s h a d y   a g r e e m e n t s   a r e   n e v e r   f a r   a w a y   i   w o u l d   s a y   t h e   m a i n   a p p e a l   o f   t h e   s h o w   i s   d u e   t o   t h e   f a c t   t h a t   i t   g o e s   w h e r e   o t h e r   s h o w s   w o u l d n   t   d a r e     f o r g e t   p r e t t y   p i c t u r e s   p a i n t e d   f o r   m a i n s t r e a m   a u d i e n c e s     f o r g e t   c h a r m     f o r g e t   r o m a n c e       o z   d o e s n   t   m e s s   a r o u n d     t h e   f i r s t   e p i s o d e   i   e v e r   s a w   s t r u c k   m e   a s   s o   n a s t y   i t   w a s   s u r r e a l     i   c o u l d n   t   s a y   i   w a s   r e a d y   f o r   i t     b u t   a s   i   w a t c h e d   m o r e     i   d e v e l o p e d   a   t a s t e   f o r   o z     a n d   g o t   a c c u s t o m e d   t o   t h e   h i g h   l e v e l s   o f   g r a p h i c   v i o l e n c e     n o t   j u s t   v i o l e n c e     b u t   i n j u s t i c e     c r o o k e d   g u a r d s   w h o   l l   b e   s o l d   o u t   f o r   a   n i c k e l     i n m a t e s   w h o   l l   k i l l   o n   o r d e r   a n d   g e t   a w a y   w i t h   i t     w e l l   m a n n e r e d     m i d d l e   c l a s s   i n m a t e s   b e i n g   t u r n e d   i n t o   p r i s o n   b i t c h e s   d u e   t o   t h e i r   l a c k   o f   s t r e e t   s k i l l s   o r   p r i s o n   e x p e r i e n c e     w a t c h i n g   o z     y o u   m a y   b e c o m e   c o m f o r t a b l e   w i t h   w h a t   i s   u n c o m f o r t a b l e   v i e w i n g         t h a t s   i f   y o u   c a n   g e t   i n   t o u c h   w i t h   y o u r   d a r k e r   s i d e  '\n",
      " 'a   w o n d e r f u l   l i t t l e   p r o d u c t i o n     t h e   f i l m i n g   t e c h n i q u e   i s   v e r y   u n a s s u m i n g     v e r y   o l d   t i m e   b b c   f a s h i o n   a n d   g i v e s   a   c o m f o r t i n g     a n d   s o m e t i m e s   d i s c o m f o r t i n g     s e n s e   o f   r e a l i s m   t o   t h e   e n t i r e   p i e c e     t h e   a c t o r s   a r e   e x t r e m e l y   w e l l   c h o s e n     m i c h a e l   s h e e n   n o t   o n l y     h a s   g o t   a l l   t h e   p o l a r i     b u t   h e   h a s   a l l   t h e   v o i c e s   d o w n   p a t   t o o     y o u   c a n   t r u l y   s e e   t h e   s e a m l e s s   e d i t i n g   g u i d e d   b y   t h e   r e f e r e n c e s   t o   w i l l i a m s     d i a r y   e n t r i e s     n o t   o n l y   i s   i t   w e l l   w o r t h   t h e   w a t c h i n g   b u t   i t   i s   a   t e r r i f i c l y   w r i t t e n   a n d   p e r f o r m e d   p i e c e     a   m a s t e r f u l   p r o d u c t i o n   a b o u t   o n e   o f   t h e   g r e a t   m a s t e r   s   o f   c o m e d y   a n d   h i s   l i f e     t h e   r e a l i s m   r e a l l y   c o m e s   h o m e   w i t h   t h e   l i t t l e   t h i n g s     t h e   f a n t a s y   o f   t h e   g u a r d   w h i c h     r a t h e r   t h a n   u s e   t h e   t r a d i t i o n a l     d r e a m     t e c h n i q u e s   r e m a i n s   s o l i d   t h e n   d i s a p p e a r s     i t   p l a y s   o n   o u r   k n o w l e d g e   a n d   o u r   s e n s e s     p a r t i c u l a r l y   w i t h   t h e   s c e n e s   c o n c e r n i n g   o r t o n   a n d   h a l l i w e l l   a n d   t h e   s e t s     p a r t i c u l a r l y   o f   t h e i r   f l a t   w i t h   h a l l i w e l l   s   m u r a l s   d e c o r a t i n g   e v e r y   s u r f a c e     a r e   t e r r i b l y   w e l l   d o n e  '\n",
      " 'i   t h o u g h t   t h i s   w a s   a   w o n d e r f u l   w a y   t o   s p e n d   t i m e   o n   a   t o o   h o t   s u m m e r   w e e k e n d     s i t t i n g   i n   t h e   a i r   c o n d i t i o n e d   t h e a t e r   a n d   w a t c h i n g   a   l i g h t   h e a r t e d   c o m e d y     t h e   p l o t   i s   s i m p l i s t i c     b u t   t h e   d i a l o g u e   i s   w i t t y   a n d   t h e   c h a r a c t e r s   a r e   l i k a b l e     e v e n   t h e   w e l l   b r e a d   s u s p e c t e d   s e r i a l   k i l l e r       w h i l e   s o m e   m a y   b e   d i s a p p o i n t e d   w h e n   t h e y   r e a l i z e   t h i s   i s   n o t   m a t c h   p o i n t   2     r i s k   a d d i c t i o n     i   t h o u g h t   i t   w a s   p r o o f   t h a t   w o o d y   a l l e n   i s   s t i l l   f u l l y   i n   c o n t r o l   o f   t h e   s t y l e   m a n y   o f   u s   h a v e   g r o w n   t o   l o v e   t h i s   w a s   t h e   m o s t   i   d   l a u g h e d   a t   o n e   o f   w o o d y   s   c o m e d i e s   i n   y e a r s     d a r e   i   s a y   a   d e c a d e         w h i l e   i   v e   n e v e r   b e e n   i m p r e s s e d   w i t h   s c a r l e t   j o h a n s o n     i n   t h i s   s h e   m a n a g e d   t o   t o n e   d o w n   h e r     s e x y     i m a g e   a n d   j u m p e d   r i g h t   i n t o   a   a v e r a g e     b u t   s p i r i t e d   y o u n g   w o m a n   t h i s   m a y   n o t   b e   t h e   c r o w n   j e w e l   o f   h i s   c a r e e r     b u t   i t   w a s   w i t t i e r   t h a n     d e v i l   w e a r s   p r a d a     a n d   m o r e   i n t e r e s t i n g   t h a n     s u p e r m a n     a   g r e a t   c o m e d y   t o   g o   s e e   w i t h   f r i e n d s  '\n",
      " ...\n",
      " 'i   a m   a   c a t h o l i c   t a u g h t   i n   p a r o c h i a l   e l e m e n t a r y   s c h o o l s   b y   n u n s     t a u g h t   b y   j e s u i t   p r i e s t s   i n   h i g h   s c h o o l       c o l l e g e     i   a m   s t i l l   a   p r a c t i c i n g   c a t h o l i c   b u t   w o u l d   n o t   b e   c o n s i d e r e d   a     g o o d   c a t h o l i c     i n   t h e   c h u r c h   s   e y e s   b e c a u s e   i   d o n   t   b e l i e v e   c e r t a i n   t h i n g s   o r   a c t   c e r t a i n   w a y s   j u s t   b e c a u s e   t h e   c h u r c h   t e l l s   m e   t o   s o   b a c k   t o   t h e   m o v i e       i t s   b a d   b e c a u s e   t w o   p e o p l e   a r e   k i l l e d   b y   t h i s   n u n   w h o   i s   s u p p o s e d   t o   b e   a   s a t i r e   a s   t h e   e m b o d i m e n t   o f   a   f e m a l e   r e l i g i o u s   f i g u r e h e a d     t h e r e   i s   n o   c o m e d y   i n   t h a t   a n d   t h e   s a t i r e   i s   n o t   d o n e   w e l l   b y   t h e   o v e r   a c t i n g   o f   d i a n e   k e a t o n     i   n e v e r   s a w   t h e   p l a y   b u t   i f   i t   w a s   v e r y   d i f f e r e n t   f r o m   t h i s   m o v i e s   t h e n   i t   m a y   b e   g o o d   a t   f i r s t   i   t h o u g h t   t h e   g u n   m i g h t   b e   a   f a k e   a n d   t h e   f i r s t   s h o o t i n g   a l l   a   p l a n   b y   t h e   f e m a l e   l e a d   o f   t h e   f o u r   f o r m e r   s t u d e n t s   a s   a n   a t t e m p t   t o   d e m o n s t r a t e   s i s t e r   m a r y   s   e m o t i o n a l   a n d   i n t e l l e c t u a l   b i g o t r y   o f   f a i t h     b u t   i t   t u r n s   o u t   t h e   b u l l e t s   w e r e   r e a l   a n d   t h e   s t o r y   h a s   t r a g e d y       t h e   t r a g e d y   o f   l o s s   o f   l i f e     b e s i d e s   t h e   t w o   f o r m e r   s t u d e n t s       t h e   l i v e s   o f   t h e   a b o r t e d   b a b i e s     t h e   l i f e   o f   t h e   s t u d e n t   s   m o m       t h e   t r a g e d y   o f   d o g m a t i c   a u t h o r i t y   o v e r   l o v e   o f   p e o p l e     t h e   t r a g e d y   o f   o r g a n i z e d   r e l i g i o n   r e p l a c i n g   t r u e   f a i t h   i n   g o d     t h i s   i s   w h a t   i s   w r o n g   w i t h   t o d a y   s   i s l a m     a n d   y e s t e r d a y   s   j u d a i s m   a n d   c h r i s t i a n i t y  '\n",
      " 'i   m   g o i n g   t o   h a v e   t o   d i s a g r e e   w i t h   t h e   p r e v i o u s   c o m m e n t   a n d   s i d e   w i t h   m a l t i n   o n   t h i s   o n e     t h i s   i s   a   s e c o n d   r a t e     e x c e s s i v e l y   v i c i o u s   w e s t e r n   t h a t   c r e a k s   a n d   g r o a n s   t r y i n g   t o   p u t   a c r o s s   i t s   c e n t r a l   t h e m e   o f   t h e   w i l d   w e s t   b e i n g   t a m e d   a n d   k i c k e d   a s i d e   b y   t h e   s t e a d y   m a r c h   o f   t i m e     i t   w o u l d   l i k e   t o   b e   i n   t h e   t r a d i t i o n   o f     b u t c h   c a s s i d y   a n d   t h e   s u n d a n c e   k i d       b u t   l a c k s   t h a t   f i l m   s   p o i g n a n c y   a n d   c h a r m     a n d r e w   m c l a g l e n   s   d i r e c t i o n   i s   l i m p     a n d   t h e   f i n a l   3 0   m i n u t e s   o r   s o   a r e   a   r e a l   b o t c h     w i t h   s o m e   i n c o m p r e h e n s i b l e   s t r a t e g y   o n   t h e   p a r t   o f   h e r o e s   c h a r l t o n   h e s t o n   a n d   c h r i s   m i t c h u m       s o m e o n e   g i v e   m e   a   h o l l e r   i f   y o u   c a n   e x p l a i n   t o   m e   w h y   t h e y   s e t   t h a t   h i l l s i d e   o n   f i r e       t h e r e   w a s   s o m e t h i n g   c a l l o u s   a b o u t   t h e   w h o l e   t r e a t m e n t   o f   t h e   r a p e   s c e n e     a n d   t h e   w o m a n   s   r e a c t i o n   a f t e r w a r d s   c e r t a i n l y   d i d   n o t   r i n g   t r u e     c o b u r n   i s   p l e n t y   n a s t y   a s   t h e   h a l f   b r e e d   e s c a p e d   c o n v i c t   o u t   f o r   r e v e n g e     b u t   a l l   o f   h i s   f e l l o w   e s c a p e e s   a r e   u n d e r d e v e l o p e d     t h e y   r e   l i k e   b o w l i n g   p i n s   t o   b e   k n o c k e d   d o w n   o n e   b y   o n e   a s   t h e   s t o r y   l u r c h e s   f o r w a r d       m i c h a e l   p a r k s   g i v e s   o n e   o f   h i s   t y p i c a l l y   s h i f t y     l e t h a r g i c     m u m b l i n g   p e r f o r m a n c e s     b u t   i n   t h i s   c a s e   i t   w a s   a p p r o p r i a t e   a s   h i s   m o d e r n   s t y l e   s h e r i f f   s y m b o l i z e s   t h e   c o m p l a c e n c y   t h a t   t e c h n o l o g i c a l   p r o g r e s s   c a n   b r i n g   a b o u t  '\n",
      " 'n o   o n e   e x p e c t s   t h e   s t a r   t r e k   m o v i e s   t o   b e   h i g h   a r t     b u t   t h e   f a n s   d o   e x p e c t   a   m o v i e   t h a t   i s   a s   g o o d   a s   s o m e   o f   t h e   b e s t   e p i s o d e s     u n f o r t u n a t e l y     t h i s   m o v i e   h a d   a   m u d d l e d     i m p l a u s i b l e   p l o t   t h a t   j u s t   l e f t   m e   c r i n g i n g       t h i s   i s   b y   f a r   t h e   w o r s t   o f   t h e   n i n e     s o   f a r     m o v i e s     e v e n   t h e   c h a n c e   t o   w a t c h   t h e   w e l l   k n o w n   c h a r a c t e r s   i n t e r a c t   i n   a n o t h e r   m o v i e   c a n   t   s a v e   t h i s   m o v i e       i n c l u d i n g   t h e   g o o f y   s c e n e s   w i t h   k i r k     s p o c k   a n d   m c c o y   a t   y o s e m i t e   i   w o u l d   s a y   t h i s   m o v i e   i s   n o t   w o r t h   a   r e n t a l     a n d   h a r d l y   w o r t h   w a t c h i n g     h o w e v e r   f o r   t h e   t r u e   f a n   w h o   n e e d s   t o   s e e   a l l   t h e   m o v i e s     r e n t i n g   t h i s   m o v i e   i s   a b o u t   t h e   o n l y   w a y   y o u   l l   s e e   i t       e v e n   t h e   c a b l e   c h a n n e l s   a v o i d   t h i s   m o v i e  ']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes : X = (40000,), y = (40000,)\n",
      "Test shapes : X = (10000,), y = (10000,)\n"
     ]
    }
   ],
   "source": [
    "trainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=9)\n",
    "print(\"Train shapes : X = {}, y = {}\".format(trainx.shape,trainy.shape))\n",
    "print(\"Test shapes : X = {}, y = {}\".format(testx.shape,testy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 't h e   l a c k   o f   c o n t e n t   i n   t h i s   m o v i e   a m a z e d   m e   t h e   m o s t     f i r s t   i   t h o u g h   t h a t   p e o p l e   w a s   g o i n g   t o   c o m p a r e   t h i s   t o   r o c k   o n     b u t   i   m   r e a l l y   s u r p r i s e d   m y s e l f   t o   s a y   t h a t   t h i s   w a s   w o r s t   t h a n   r o c k   o n     s o   s o   s t o r y   h o r r i b l e   c a s t   a j a y   d e v g a n   j a m m i n g   w i t h   s a l m a n   k h a n   a n d   a s i n   y o u   g o t t a   b e   k i d d i n g   m e     t h e   m u s i c   w a s   o k a y   k h a n a b a d o s h   w a s   t h e   t r a c k   o f   t h e   m o v i e   t h e   r e s t   w a s   b a d     v i p u l   s h a h   h a s n   t   s t i l l   l e a r n   f r o m   s i n g h   i s   k i n g   s   c r i t i c a l l y   b a s h e d   c o m e d y     n o w   a s i n       w h e r e   d o   s h e   c o m e   f r o m   s o r r y   f o r   a s i n   s   f a n   o u t   t h e r e   b u t   s h e   s u c k   d   b i g   t i m e   i n   t h i s   m o v i e   s e r i o u s l y   b a d   a c t i n g   s h e   d i d n   t   l o o k   g o o d   a t   a l l   o v e r d o s e   o f   m a k e   u p     m y   f i n a l   v e r d i c t   g o   w a t c h   a l a d i n   w i t h   y o u r   f a m i l y   i n s t e a d   w a s t i n g   y o u r   t i m e   h e r e  '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gnb,mnb,bnb \u001b[39m=\u001b[39m GaussianNB(),MultinomialNB(alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,fit_prior\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),BernoulliNB(alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,fit_prior\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m gnb\u001b[39m.\u001b[39;49mfit(trainx,trainy)\n\u001b[0;32m      3\u001b[0m mnb\u001b[39m.\u001b[39mfit(trainx,trainy)\n\u001b[0;32m      4\u001b[0m bnb\u001b[39m.\u001b[39mfit(trainx,trainy)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:267\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    266\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(y\u001b[39m=\u001b[39my)\n\u001b[1;32m--> 267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    268\u001b[0m     X, y, np\u001b[39m.\u001b[39;49munique(y), _refit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    269\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:426\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    425\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[1;32m--> 426\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    553\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    555\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1104\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[1;32m-> 1104\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1105\u001b[0m     X,\n\u001b[0;32m   1106\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1107\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1108\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1109\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1110\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1111\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1112\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1113\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1114\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1115\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1116\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1117\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1118\u001b[0m )\n\u001b[0;32m   1120\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1122\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 't h e   l a c k   o f   c o n t e n t   i n   t h i s   m o v i e   a m a z e d   m e   t h e   m o s t     f i r s t   i   t h o u g h   t h a t   p e o p l e   w a s   g o i n g   t o   c o m p a r e   t h i s   t o   r o c k   o n     b u t   i   m   r e a l l y   s u r p r i s e d   m y s e l f   t o   s a y   t h a t   t h i s   w a s   w o r s t   t h a n   r o c k   o n     s o   s o   s t o r y   h o r r i b l e   c a s t   a j a y   d e v g a n   j a m m i n g   w i t h   s a l m a n   k h a n   a n d   a s i n   y o u   g o t t a   b e   k i d d i n g   m e     t h e   m u s i c   w a s   o k a y   k h a n a b a d o s h   w a s   t h e   t r a c k   o f   t h e   m o v i e   t h e   r e s t   w a s   b a d     v i p u l   s h a h   h a s n   t   s t i l l   l e a r n   f r o m   s i n g h   i s   k i n g   s   c r i t i c a l l y   b a s h e d   c o m e d y     n o w   a s i n       w h e r e   d o   s h e   c o m e   f r o m   s o r r y   f o r   a s i n   s   f a n   o u t   t h e r e   b u t   s h e   s u c k   d   b i g   t i m e   i n   t h i s   m o v i e   s e r i o u s l y   b a d   a c t i n g   s h e   d i d n   t   l o o k   g o o d   a t   a l l   o v e r d o s e   o f   m a k e   u p     m y   f i n a l   v e r d i c t   g o   w a t c h   a l a d i n   w i t h   y o u r   f a m i l y   i n s t e a d   w a s t i n g   y o u r   t i m e   h e r e  '"
     ]
    }
   ],
   "source": [
    "gnb,mnb,bnb = GaussianNB(),MultinomialNB(alpha=1.0,fit_prior=True),BernoulliNB(alpha=1.0,fit_prior=True)\n",
    "gnb.fit(trainx,trainy)\n",
    "mnb.fit(trainx,trainy)\n",
    "bnb.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 's o m e   p e o p l e   s e e m   t o   t h i n k   t h i s   w a s   t h e   w o r s t   m o v i e   t h e y   h a v e   e v e r   s e e n     a n d   i   u n d e r s t a n d   w h e r e   t h e y   r e   c o m i n g   f r o m     b u t   i   r e a l l y   h a v e   s e e n   w o r s e   t h a t   b e i n g   s a i d     t h e   m o v i e s   t h a t   i   c a n   r e c a l l     i e   t h e   o n e s   i   h a v e n   t   b l o c k e d   o u t     t h a t   w e r e   w o r s e   t h a n   t h i s     w e r e   s o   b a d   t h a t   t h e y   p h y s i c a l l y   p a i n e d   e v e r y   s e n s e   t h a t   w a s   i n v o l v e d   w i t h   w a t c h i n g   t h e   m o v i e     t h e   m o v i e s   t h a t   a r e   w o r s e   t h a n   w a r   g a m e s   2   a r e   t h e   o n e s   t h a t   m a k e   y o u   w a n t   t o   g o u g e   o u t   y o u r   e y e s     o r   s t a b   s h a r p   o b j e c t s   i n   y o u r   e a r s   t o   k e e p   y o u r s e l f   f r o m   h a v i n g   a n o t h e r   p i e c e   o f   y o u r   s o u l   r i p p e d   a w a y   f r o m   y o u   b y   t h e   a w f u l n e s s   w a r   g a m e s     t h e   d e a d   c o d e   i s n   t   t h a t   b a d     b u t   i t   c o m e s   p r e t t y   c l o s e     y e s   i   w a s   a   f a n   o f   t h e   o r i g i n a l     b u t   n o   i   w a s n   t   e x p e c t i n g   m i r a c l e s   f r o m   t h i s   o n e     l e t   s   f a c e   i t   t h e   o r i g i n a l   w a s n   t   r e a l l y   t h a t   g r e a t   o f   a   m o v i e   i n   t h e   f i r s t   p l a c e     i t   w a s   b a s i c a l l y   j u s t   a   c a m p y   8 0 s   t e e n   r o m a n c e   f l i c k   w i t h   s o m e   g e e k   a p p e a l   t o   i t   t h a t   s   a l l   i   w a s   h o p i n g   f o r     s o m e t h i n g   b a d     b u t   t h a t   m i g h t   h a v e   t u g g e d   a t   m y   g e e k   s t r i n g s     w a s   t h a t   t o o   m u c h   t o   a s k   f o r     i s   i t   r e a l l y   n o t   p o s s i b l e   t o   d o   b e t t e r   t h a n   t h e   o r i g i n a l   w a r   g a m e s     e v e n   f o r   a   s t r a i g h t   t o   v i d e o   r e l e a s e     w e l l   a p p a r e n t l y   t h a t   w a s   t o o   m u c h   t o   a s k   f o r     s t a y   a w a y   f r o m   t h i s   m o v i e     a t   f i r s t   i t   s   j u s t   b a d     l i k e     o h   y e a h     t h i s   i s   b a d     b u t   i   m   k i n d   o f   e n j o y i n g   i t     m a y b e   t h e   e n d   w i l l   b e   g o o d   l i k e   i n   t h e   o r i g i n a l       a n d   t h e n   i t   j u s t   g e t s   w o r s e   a n d   w o r s e     a n d   b y   t h e   e n d     t r u s t   m e     y o u   w i l l   w i s h   y o u   h a d   n o t   s e e n   t h i s   m o v i e  '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ypg \u001b[39m=\u001b[39m gnb\u001b[39m.\u001b[39;49mpredict(testx)\n\u001b[0;32m      2\u001b[0m ypm \u001b[39m=\u001b[39m mnb\u001b[39m.\u001b[39mpredict(testx)\n\u001b[0;32m      3\u001b[0m ypb \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mpredict(testx)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:105\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(X)\n\u001b[0;32m    106\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:273\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    272\u001b[0m     \u001b[39m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 's o m e   p e o p l e   s e e m   t o   t h i n k   t h i s   w a s   t h e   w o r s t   m o v i e   t h e y   h a v e   e v e r   s e e n     a n d   i   u n d e r s t a n d   w h e r e   t h e y   r e   c o m i n g   f r o m     b u t   i   r e a l l y   h a v e   s e e n   w o r s e   t h a t   b e i n g   s a i d     t h e   m o v i e s   t h a t   i   c a n   r e c a l l     i e   t h e   o n e s   i   h a v e n   t   b l o c k e d   o u t     t h a t   w e r e   w o r s e   t h a n   t h i s     w e r e   s o   b a d   t h a t   t h e y   p h y s i c a l l y   p a i n e d   e v e r y   s e n s e   t h a t   w a s   i n v o l v e d   w i t h   w a t c h i n g   t h e   m o v i e     t h e   m o v i e s   t h a t   a r e   w o r s e   t h a n   w a r   g a m e s   2   a r e   t h e   o n e s   t h a t   m a k e   y o u   w a n t   t o   g o u g e   o u t   y o u r   e y e s     o r   s t a b   s h a r p   o b j e c t s   i n   y o u r   e a r s   t o   k e e p   y o u r s e l f   f r o m   h a v i n g   a n o t h e r   p i e c e   o f   y o u r   s o u l   r i p p e d   a w a y   f r o m   y o u   b y   t h e   a w f u l n e s s   w a r   g a m e s     t h e   d e a d   c o d e   i s n   t   t h a t   b a d     b u t   i t   c o m e s   p r e t t y   c l o s e     y e s   i   w a s   a   f a n   o f   t h e   o r i g i n a l     b u t   n o   i   w a s n   t   e x p e c t i n g   m i r a c l e s   f r o m   t h i s   o n e     l e t   s   f a c e   i t   t h e   o r i g i n a l   w a s n   t   r e a l l y   t h a t   g r e a t   o f   a   m o v i e   i n   t h e   f i r s t   p l a c e     i t   w a s   b a s i c a l l y   j u s t   a   c a m p y   8 0 s   t e e n   r o m a n c e   f l i c k   w i t h   s o m e   g e e k   a p p e a l   t o   i t   t h a t   s   a l l   i   w a s   h o p i n g   f o r     s o m e t h i n g   b a d     b u t   t h a t   m i g h t   h a v e   t u g g e d   a t   m y   g e e k   s t r i n g s     w a s   t h a t   t o o   m u c h   t o   a s k   f o r     i s   i t   r e a l l y   n o t   p o s s i b l e   t o   d o   b e t t e r   t h a n   t h e   o r i g i n a l   w a r   g a m e s     e v e n   f o r   a   s t r a i g h t   t o   v i d e o   r e l e a s e     w e l l   a p p a r e n t l y   t h a t   w a s   t o o   m u c h   t o   a s k   f o r     s t a y   a w a y   f r o m   t h i s   m o v i e     a t   f i r s t   i t   s   j u s t   b a d     l i k e     o h   y e a h     t h i s   i s   b a d     b u t   i   m   k i n d   o f   e n j o y i n g   i t     m a y b e   t h e   e n d   w i l l   b e   g o o d   l i k e   i n   t h e   o r i g i n a l       a n d   t h e n   i t   j u s t   g e t s   w o r s e   a n d   w o r s e     a n d   b y   t h e   e n d     t r u s t   m e     y o u   w i l l   w i s h   y o u   h a d   n o t   s e e n   t h i s   m o v i e  '"
     ]
    }
   ],
   "source": [
    "ypg = gnb.predict(testx)\n",
    "ypm = mnb.predict(testx)\n",
    "ypb = bnb.predict(testx)\n",
    "\n",
    "print(\"Gaussian = \",accuracy_score(testy,ypg))\n",
    "print(\"Multinomial = \",accuracy_score(testy,ypm))\n",
    "print(\"Bernoulli = \",accuracy_score(testy,ypb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bnb,open('model1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m f2 \u001b[39m=\u001b[39m is_special(f1)\n\u001b[0;32m     10\u001b[0m f3 \u001b[39m=\u001b[39m to_lower(f2)\n\u001b[1;32m---> 11\u001b[0m f4 \u001b[39m=\u001b[39m rem_stopwords(f3)\n\u001b[0;32m     12\u001b[0m f5 \u001b[39m=\u001b[39m stem_txt(f4)\n\u001b[0;32m     14\u001b[0m bow,words \u001b[39m=\u001b[39m [],word_tokenize(f5)\n",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m, in \u001b[0;36mrem_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrem_stopwords\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nayak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nayak/nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nayak\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nayak\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "rev =  \"\"\"Terrible. Complete trash. Brainless tripe. Insulting to anyone who isn't an 8 year old fan boy. Im actually pretty disgusted that this movie is making the money it is - what does it say about the people who brainlessly hand over the hard earned cash to be 'entertained' in this fashion and then come here to leave a positive 8.8 review?? Oh yes, they are morons. Its the only sensible conclusion to draw. How anyone can rate this movie amongst the pantheon of great titles is beyond me.\n",
    "\n",
    "So trying to find something constructive to say about this title is hard...I enjoyed Iron Man? Tony Stark is an inspirational character in his own movies but here he is a pale shadow of that...About the only 'hook' this movie had into me was wondering when and if Iron Man would knock Captain America out...Oh how I wished he had :( What were these other characters anyways? Useless, bickering idiots who really couldn't organise happy times in a brewery. The film was a chaotic mish mash of action elements and failed 'set pieces'...\n",
    "\n",
    "I found the villain to be quite amusing.\n",
    "\n",
    "And now I give up. This movie is not robbing any more of my time but I felt I ought to contribute to restoring the obvious fake rating and reviews this movie has been getting on IMDb.\"\"\"\n",
    "f1 = clean(rev)\n",
    "f2 = is_special(f1)\n",
    "f3 = to_lower(f2)\n",
    "f4 = rem_stopwords(f3)\n",
    "f5 = stem_txt(f4)\n",
    "\n",
    "bow,words = [],word_tokenize(f5)\n",
    "for word in words:\n",
    "    bow.append(words.count(word))\n",
    "#np.array(bow).reshape(1,3000)\n",
    "#bow.shape\n",
    "word_dict = cv.vocabulary_\n",
    "pickle.dump(word_dict,open('bow.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inp \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m word_dict:\n\u001b[0;32m      3\u001b[0m     inp\u001b[39m.\u001b[39mappend(f5\u001b[39m.\u001b[39mcount(i[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      4\u001b[0m y_pred \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39marray(inp)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m1000\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_dict' is not defined"
     ]
    }
   ],
   "source": [
    "inp = []\n",
    "for i in word_dict:\n",
    "    inp.append(f5.count(i[0]))\n",
    "y_pred = bnb.predict(np.array(inp).reshape(1,1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd354288a369f680a7322ddff2a06d3b3ac546c5fe22b956c0c96b589b2c4d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
